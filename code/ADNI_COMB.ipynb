{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADNI_COMB 1.0 09/24/2019\n",
    "##### Xiao Gao, Department of Radiology and Biomedical Imaging, UCSF; Myriam Chaumeil Lab (xiao.gao@ucsf.edu; xiao.gao@berkeley.edu)  \n",
    "\n",
    "\n",
    "### Purpose: Reorganizing the latest open-sourced ADNI csv. files from http://adni.loni.usc.edu in Python environment; focusing on imaging data\n",
    "\n",
    "### Prerequisite: Authorized access to ADNI database and having dowloaded necessary csv. files \n",
    "\n",
    "### Data Framework: Mainly based on adnimerge.csv by Michael C. Donohue, et al. from UCSD\n",
    "\n",
    "### Notice: This document is presented by the author(s) as a service to ADNI data users. However, users should be aware that no formal review process has vetted this document and that ADNI cannot guarantee the accuracy or utility of this document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dumb function 1) locating the column of datatime in one pandas dateframe\n",
    "#                 2) converting string datetime to Gregorian ordinal datetime\n",
    "#                 3) adding the new numerical datetime as another column of one dataframe\n",
    "#                 4) dropping rows without datetime information \n",
    "#                 5) returning the dataframe with alias prefix added\n",
    "\n",
    "# Inputs: 1) df = one pandas dataframe read from an ADNI csv. file\n",
    "#         2) alias = nickname of raw csv. file for convenience purposes in nameing and calling\n",
    "#         3) date_entry_name = the specific column name of 'exam date' or 'scan date' of the raw csv. file\n",
    "#         4) date_format = the format of input dateframe's datetime format. \n",
    "#            Usually, when generated via pd.read_csv method, the default datetime format is \"%Y-%m-%d\".\n",
    "\n",
    "# Attention: input information about 'df', 'alias', and 'date_entry_name' is accessible from dataframe 'comb_df'\n",
    "\n",
    "#_you_can_customize_the_numerical-datetime_column_name_here:\n",
    "numdate_col = '_numdate'\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "def add_numerical_date(df, date_entry_name, alias='new', date_format = \"%Y-%m-%d\", numdate_col=numdate_col):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    temp_date = df[date_entry_name]\n",
    "    temp_numdate = np.zeros_like(temp_date)\n",
    "    \n",
    "    for i in range(len(temp_date)):\n",
    "        date = temp_date[i]\n",
    "        if type(date) == str:        \n",
    "            if date[0:2]=='00':\n",
    "                date='2'+date[1::] # handle dating problems like '0012-12-25' transferred from '12/12/25'    \n",
    "            date = pd.to_datetime(date).date()\n",
    "            date = str(date)\n",
    "            df.loc[i, alias+numdate_col]= datetime.strptime(date, date_format).toordinal()\n",
    "        else:\n",
    "            df.drop(i, inplace=True)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = ['SV', 'CV', 'SA', 'TA', 'TS', 'HS',\\\n",
    "                'MIN', 'MAX', 'AVG', 'SD', 'CT', 'MD',\\\n",
    "                'SUVR', 'VOLUME']\n",
    "                # 'FA', 'MD', 'RD', 'AD',\n",
    "\n",
    "def change_naming_convention(df, nm_pd, metrics_dict):\n",
    "       \n",
    "    for old_column in df.columns[100:4320]:\n",
    "        new_column = old_column\n",
    "        prefix = re.split('_', old_column)[0]\n",
    "        number_convention = False\n",
    "        string_convention = False\n",
    "        whether_region_column  = False\n",
    "        \n",
    "        for nn in metrics_dict:\n",
    "                # only brain region entries have specific metric names by the end of column name\n",
    "                if bool(re.search('(?<![A-Z])'+nn+ '(?![A-Z])', old_column)):\n",
    "                    whether_region_column = True\n",
    "                    metric = nn\n",
    "        \n",
    "        if bool(re.search('ST[0-9]', old_column)): # tell numerical naming convention from string naming convention\n",
    "            number_convention = True # some naming convention like 'ST39CV' or 'ST1SV'\n",
    "        elif whether_region_column: \n",
    "            string_convention = True # some naming convention like 'LeftCaudalAnteriorCingulate_SUVR'\n",
    "        \n",
    "        \n",
    "        if number_convention:\n",
    "            \n",
    "            try:\n",
    "                # old_region = the string between '$prefix$_\" and '$metric$'         \n",
    "                old_region = re.search('(?<=' + prefix + '_)\\w+(?=' + metric + ')', old_column).group(0)\n",
    "                new_region = old_region\n",
    "                for conv in nm_pd.columns:\n",
    "                    if old_region in nm_pd[conv].values:\n",
    "                        new_region = nm_pd.loc[nm_pd[conv]==old_region]['conv_comb'].values[0]\n",
    "                new_column = prefix + '_' + new_region + '_' + metric\n",
    "            except:\n",
    "                print(old_column)\n",
    "                print(metric)    \n",
    "    \n",
    "        if string_convention:\n",
    "            \n",
    "            try:\n",
    "                # old_region = the text between '$prefix$_\" and '_$metric$'\n",
    "                old_region = re.search('(?<=' + prefix + '_)\\w+(?=_' + metric + ')', old_column).group(0)\n",
    "                new_region = old_region\n",
    "                for conv in nm_pd.columns:\n",
    "                    if old_region in nm_pd[conv].values:\n",
    "                        new_region = nm_pd.loc[nm_pd[conv]==old_region]['conv_comb'].values[0]\n",
    "                new_column = prefix + '_' + new_region + '_' + metric         \n",
    "            except:\n",
    "                print(old_column)\n",
    "                print(metric)                     \n",
    "                        \n",
    "        df.rename(columns = {old_column:new_column}, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Checking downloaded csv. files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.dirname(os.getcwd()) # working from the upper directory of this .ipynd file\n",
    "for root, dirs, files in os.walk(working_dir):\n",
    "    for name in files:\n",
    "        if name==\"adnicomb_list.csv\":\n",
    "            comb_list = root + os.sep + name\n",
    "        if name==\"adnicomb_naming_convention.csv\":\n",
    "            nm_conv = root + os.sep + name\n",
    "        \n",
    "comb_df = pd.read_csv(comb_list, usecols=['csv', 'alias', 'date_entry', 'subject_entry', 'subject_type','recruit'])\n",
    "nm_pd = pd.read_csv(nm_conv, usecols=['conv1', 'conv2', 'conv3', 'conv_comb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Locating csv. files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csv</th>\n",
       "      <th>alias</th>\n",
       "      <th>date_entry</th>\n",
       "      <th>subject_entry</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>recruit</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADNIMERGE.csv</td>\n",
       "      <td>merge</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/merge/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MRILIST.csv</td>\n",
       "      <td>mrimeta</td>\n",
       "      <td>SCANDATE</td>\n",
       "      <td>SUBJECT</td>\n",
       "      <td>PTID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/mr_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PET_META_LIST.csv</td>\n",
       "      <td>petmeta</td>\n",
       "      <td>Scan Date</td>\n",
       "      <td>Subject</td>\n",
       "      <td>PTID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/pet/PE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADNI_HULAB.csv</td>\n",
       "      <td>csfhu</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/biomk/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UPENNBIOMK9_04_19_17.csv</td>\n",
       "      <td>csfup9</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/biomk/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UPENNBIOMK10_07_29_19.csv</td>\n",
       "      <td>csfupx</td>\n",
       "      <td>DRAWDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/biomk/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DESIKANLAB.csv</td>\n",
       "      <td>desikan</td>\n",
       "      <td>STATIC</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/geneti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UCSFFSL_02_01_16.csv</td>\n",
       "      <td>lv15one</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UCSFFSL51ALL_08_01_16.csv</td>\n",
       "      <td>lv30two</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UCSFFSX_11_02_15.csv</td>\n",
       "      <td>xv15one</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UCSFFSX51_ADNI1_3T_02_01_16.csv</td>\n",
       "      <td>xv30one</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>UCSFFSX51_08_27_19.csv</td>\n",
       "      <td>xv30two</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UCSFFSX6_08_27_19.csv</td>\n",
       "      <td>xv3three</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MAYOADIRL_MRI_FMRI_NFQ_09_17_19.csv</td>\n",
       "      <td>dmn</td>\n",
       "      <td>SCANDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MRI_INFARCTS_11_16_15.csv</td>\n",
       "      <td>infarct</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/mri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ADNI_DTIROI_04_30_19.csv</td>\n",
       "      <td>dti</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UCSFASLFS_11_02_15_V2.csv</td>\n",
       "      <td>asl</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TBM_08_20_15.csv</td>\n",
       "      <td>tbm</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/mr/usc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>UCBERKELEYAV1451_08_27_19.csv</td>\n",
       "      <td>taunpvc</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/pet/uc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UCBERKELEYAV1451_PVC_08_27_19.csv</td>\n",
       "      <td>tauwpvc</td>\n",
       "      <td>EXAMDATE</td>\n",
       "      <td>RID</td>\n",
       "      <td>RID</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/gavin/Documents/raj_lab/ADNI_COMB/pet/uc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    csv     alias date_entry subject_entry  \\\n",
       "0                         ADNIMERGE.csv     merge   EXAMDATE           RID   \n",
       "1                           MRILIST.csv   mrimeta   SCANDATE       SUBJECT   \n",
       "2                     PET_META_LIST.csv   petmeta  Scan Date       Subject   \n",
       "3                        ADNI_HULAB.csv     csfhu   EXAMDATE           RID   \n",
       "4              UPENNBIOMK9_04_19_17.csv    csfup9   EXAMDATE           RID   \n",
       "5             UPENNBIOMK10_07_29_19.csv    csfupx   DRAWDATE           RID   \n",
       "6                        DESIKANLAB.csv   desikan     STATIC           RID   \n",
       "7                  UCSFFSL_02_01_16.csv   lv15one   EXAMDATE           RID   \n",
       "8             UCSFFSL51ALL_08_01_16.csv   lv30two   EXAMDATE           RID   \n",
       "9                  UCSFFSX_11_02_15.csv   xv15one   EXAMDATE           RID   \n",
       "10      UCSFFSX51_ADNI1_3T_02_01_16.csv   xv30one   EXAMDATE           RID   \n",
       "11               UCSFFSX51_08_27_19.csv   xv30two   EXAMDATE           RID   \n",
       "12                UCSFFSX6_08_27_19.csv  xv3three   EXAMDATE           RID   \n",
       "13  MAYOADIRL_MRI_FMRI_NFQ_09_17_19.csv       dmn   SCANDATE           RID   \n",
       "14            MRI_INFARCTS_11_16_15.csv   infarct   EXAMDATE           RID   \n",
       "15             ADNI_DTIROI_04_30_19.csv       dti   EXAMDATE           RID   \n",
       "16            UCSFASLFS_11_02_15_V2.csv       asl   EXAMDATE           RID   \n",
       "17                     TBM_08_20_15.csv       tbm   EXAMDATE           RID   \n",
       "18        UCBERKELEYAV1451_08_27_19.csv   taunpvc   EXAMDATE           RID   \n",
       "19    UCBERKELEYAV1451_PVC_08_27_19.csv   tauwpvc   EXAMDATE           RID   \n",
       "\n",
       "   subject_type  recruit                                                dir  \n",
       "0           RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/merge/...  \n",
       "1          PTID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/mr_...  \n",
       "2          PTID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/pet/PE...  \n",
       "3           RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/biomk/...  \n",
       "4           RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/biomk/...  \n",
       "5           RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/biomk/...  \n",
       "6           RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/geneti...  \n",
       "7           RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...  \n",
       "8           RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...  \n",
       "9           RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...  \n",
       "10          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...  \n",
       "11          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...  \n",
       "12          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...  \n",
       "13          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/may...  \n",
       "14          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/mri...  \n",
       "15          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucl...  \n",
       "16          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/ucs...  \n",
       "17          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/mr/usc...  \n",
       "18          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/pet/uc...  \n",
       "19          RID        1  /home/gavin/Documents/raj_lab/ADNI_COMB/pet/uc...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(working_dir):\n",
    "    for name in files:\n",
    "        if name.endswith((\".csv\")):\n",
    "            comb_df.loc[comb_df.csv == name, 'dir'] = root + os.sep + name\n",
    "\n",
    "comb_df.head(20)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stacking dataframes over/alongside ADNIMERGE.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gavin/Library/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (18,19,20,103,104) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort size of all ADNI phases:  2175\n"
     ]
    }
   ],
   "source": [
    "merge_alias = comb_df.loc[comb_df.csv == 'ADNIMERGE.csv', 'alias'].to_list()[0]\n",
    "\n",
    "# import ADNIMERGE.csv as the framework/concordance of whole adnicomb dataset in format of pandas dateframe\n",
    "adnicomb = (pd.read_csv(comb_df.loc[comb_df.alias==merge_alias,'dir'].to_list()[0])\n",
    "              .add_prefix(merge_alias+'_')) # add prefix (the .csv's alias) to the columns\n",
    "\n",
    "# add one column of Gregorian ordinal datetime\n",
    "adnicomb = add_numerical_date(adnicomb, merge_alias+'_EXAMDATE', merge_alias,  '%Y-%m-%d')\n",
    "\n",
    "# sort the dataframe in ascending order of RID as well as EXAMDATE\n",
    "adnicomb.sort_values(by=[merge_alias+'_RID', merge_alias+'_EXAMDATE'], ascending=[1, 1], inplace = True)\n",
    "adnicomb.reset_index(inplace = True, drop=True)\n",
    "adnicomb_index =  adnicomb.index\n",
    "\n",
    "# get lists of unique ADNI-1, Go, 2 and 3 subjects' RID and PTID\n",
    "adni_rid_uniq = adnicomb[merge_alias+'_RID'].unique()\n",
    "adni_ptid_uniq = adnicomb[merge_alias+'_PTID'].unique()\n",
    "\n",
    "print('Cohort size of all ADNI phases: ', len(adni_rid_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv. files:\n",
      "...  merge ,\n",
      "    with using  0.00  seconds ...\n",
      "...  mrimeta ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gavin/Library/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    with using  1332.53  seconds ...\n",
      "...  petmeta ,\n",
      "    with using  708.45  seconds ...\n",
      "...  csfhu ,\n",
      "    with using  76.02  seconds ...\n",
      "...  csfup9 ,\n",
      "    with using  246.99  seconds ...\n",
      "...  csfupx ,\n",
      "    with using  49.10  seconds ...\n",
      "...  desikan ,\n",
      "    with using  84.66  seconds ...\n",
      "...  lv15one ,\n",
      "    with using  7132.26  seconds ...\n",
      "...  lv30two ,\n",
      "    with using  3959.64  seconds ...\n",
      "...  xv15one ,\n",
      "    with using  8118.86  seconds ...\n",
      "...  xv30one ,\n",
      "    with using  1095.87  seconds ...\n",
      "...  xv30two ,\n",
      "    with using  8354.22  seconds ...\n",
      "...  xv3three ,\n",
      "    with using  1866.51  seconds ...\n",
      "...  dmn ,\n",
      "    with using  405.42  seconds ...\n",
      "...  infarct ,\n",
      "    with using  529.27  seconds ...\n",
      "...  dti ,\n",
      "    with using  1121.73  seconds ...\n",
      "...  asl ,\n",
      "    with using  2903.65  seconds ...\n",
      "...  tbm ,\n",
      "    with using  562.56  seconds ...\n",
      "...  taunpvc ,\n",
      "    with using  1722.24  seconds ...\n",
      "...  tauwpvc ,\n",
      "    with using  1179.91  seconds ...\n",
      "...  av45 ,\n",
      "    with using  4434.88  seconds ...\n",
      "...  fbb ,\n",
      "    with using  553.55  seconds ...\n",
      "...  fdg ,\n",
      "    with using  641.42  seconds ...\n",
      "...  purine ,\n",
      "    with using  113.13  seconds ...\n",
      "...  bileacid ,\n",
      "    with using  916.41  seconds ...\n",
      "...  drug ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gavin/Library/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (3,11,16,32,33,38,52,66,67,72,84,87) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    with using  7645.55  seconds ...\n",
      "...  p180fi1 ,\n",
      "    with using  1365.90  seconds ...\n",
      "...  p180fi2 ,\n",
      "    with using  1681.35  seconds ...\n",
      "...  p180lc1 ,\n",
      "    with using  471.61  seconds ...\n",
      "...  p180lc2 ,\n",
      "    with using  671.82  seconds ...\n",
      "...  lipopro ,\n",
      "    with using  4552.89  seconds ...\n",
      "...  etn ,\n",
      "    with using  809.50  seconds ...\n",
      "...  apoe ,\n",
      "    with using  381.05  seconds ...\n",
      "...  ab42_40 ,\n",
      "    with using  203.81  seconds ...\n",
      "...  nfl1 ,\n",
      "    with using  67.24  seconds ...\n",
      "...  nfl2 ,\n",
      "    with using  287.62  seconds ...\n",
      "...  csfzhang ,\n",
      "    with using  89.42  seconds ...\n",
      "...  s_trem ,\n",
      "    with using  238.78  seconds ...\n",
      "...  csfeuro ,\n",
      "    with using  20.32  seconds ...\n",
      "...  csffagan ,\n",
      "    with using  67.89  seconds ...\n",
      "...  msms1 ,\n",
      "    with using  41.99  seconds ...\n",
      "...  msms2 ,\n",
      "    with using  127.07  seconds ...\n"
     ]
    }
   ],
   "source": [
    "print('Reading csv. files:') \n",
    "for alias in comb_df.loc[comb_df.recruit==1]['alias']:\n",
    "    start = time.time()\n",
    "    flag_dupl=0\n",
    "    print('... ',alias, ',')    \n",
    "    if alias != merge_alias: # we have already import ADNIMERGE.csv\n",
    "        csv_df =  (pd.read_csv(comb_df.loc[comb_df.alias==alias, 'dir'].to_list()[0])\n",
    "                     .add_prefix(alias+'_'))\n",
    "        \n",
    "        date_entry = alias+'_'+comb_df.loc[comb_df.alias==alias, 'date_entry'].to_list()[0]\n",
    "        subject_entry = alias+'_'+comb_df.loc[comb_df.alias==alias, 'subject_entry'].to_list()[0]\n",
    "        subject_type = comb_df.loc[comb_df.alias==alias, 'subject_type'].to_list()[0]     \n",
    "        \n",
    "        # Here below, we use subject id and exam date as two different indices to locate clinical visit\n",
    "        if subject_type == 'RID':\n",
    "            index1 = merge_alias + '_RID'\n",
    "            merge_subj_uniq = adni_rid_uniq\n",
    "\n",
    "        elif subject_type == 'PTID':\n",
    "            index1 = merge_alias + '_PTID'\n",
    "            merge_subj_uniq = adni_ptid_uniq\n",
    "            \n",
    "        if 'VISCODE' in date_entry:\n",
    "            index2 = merge_alias + '_VISCODE'\n",
    "        elif 'STATIC' in date_entry:\n",
    "            index2 = index1\n",
    "            date_entry = subject_entry \n",
    "        else:\n",
    "            index2 = merge_alias + numdate_col\n",
    "            # if date type is numerical, add one column of Gregorian ordinal datetime to csv-of-interest\n",
    "            csv_df = add_numerical_date(csv_df, date_entry, alias, \"%Y-%m-%d\")\n",
    "            date_entry = alias + numdate_col \n",
    "        \n",
    "        # initiate new columns from csv_df\n",
    "        for new_col in csv_df.columns:\n",
    "            #adnicomb[new_col]= np.nan  \n",
    "            adnicomb[new_col]= np.full_like(np.empty((adnicomb.shape[0],1,)), np.nan).tolist()\n",
    "        \n",
    "        subj_date_merge = adnicomb[[index1,index2]]\n",
    "        \n",
    "        # unique csv. file subject IDs\n",
    "        if any(csv_df[subject_entry].duplicated()):\n",
    "            flag_dupl=1\n",
    "        subject_uniq = csv_df[subject_entry].unique()\n",
    "        \n",
    "        # iterating through all patient_of_interests' IDs         \n",
    "        for subject in subject_uniq:\n",
    "            for new_col in csv_df.columns:\n",
    "                 # For some screening studies, it is possible to find subjects of csv-of-interest abscent from ADNIMERGE.csv\n",
    "                if subject in merge_subj_uniq:\n",
    "                   # patient-of-interest's all clinical visit dates from ADNIMERGE.csv\n",
    "                    try:\n",
    "                        date_merge = subj_date_merge.loc[(subj_date_merge[index1]==subject).iloc[:,0]][index2].iloc[:,0]\n",
    "                    except:\n",
    "                        date_merge = subj_date_merge.loc[subj_date_merge[index1]==subject][index2]\n",
    "                  \n",
    "                    # patient-of-interest's all clinical visit information from csv-of-interest\n",
    "                    col_csv = csv_df.loc[csv_df[subject_entry]==subject]\n",
    "                    col_csv = col_csv.reset_index(drop=True)\n",
    "                    col_csv.loc[col_csv[date_entry]=='sc',[date_entry]]='bl'  # consider screen visit as baseline visit\n",
    "                  \n",
    "                    date_csv = col_csv[date_entry].to_frame()\n",
    "                    date_csv = (date_csv.reset_index(drop=True)\n",
    "                                      .rename(columns={date_entry: \"date\"}))\n",
    "                  \n",
    "                    # initiate enrollment_datetime array, connecting date_csv to date_merge\n",
    "                    date_enrol = pd.DataFrame(np.zeros_like(date_csv), columns = ['date'])\n",
    "                  \n",
    "                  \n",
    "                  # an if-loop locating the scan/measure/draw date(s) best matching the record of ADNIMERGE.csv\n",
    "                  # Attension: it's possible that several rows from csv-of-interest corrspond to one single row of\n",
    "                  # ADNIMERGE.csv, so we need iterate through all dates from csv-of-interest to figure it out\n",
    "                    for date1 in date_csv.date:\n",
    "                      \n",
    "                        if index2 == merge_alias + numdate_col:          \n",
    "                            delta_date = np.absolute(date1 - date_merge)\n",
    "                            enrol_index =  date_csv.date.loc[date_csv.date==date1].index.values\n",
    "                            # dates from csv-of-interest are absorbed to the closest date from ADNIMERGE.csv\n",
    "                            date_enrol.date.loc[enrol_index] = date_merge.iloc[np.argmin(delta_date.values)]               \n",
    "                        else:\n",
    "                          # if the datatime type is ordinal visit number, just find the same date string in ADNIMERGE.csv\n",
    "                            if np.sum(date_merge == date1):  # if date1 in date_merge                         \n",
    "                                date_merge_unique = date_merge.loc[date_merge==date1].iloc[0]\n",
    "                                enrol_index =  date_csv.date.loc[date_csv.date==date1].index.values\n",
    "                                date_enrol.date.loc[enrol_index] = date_merge_unique\n",
    "                            else:\n",
    "                                # drop the csv date not existing in merge date, which rarely happens except for screening visits\n",
    "                                drop_index = date_csv.date.loc[date_csv.date==date1].index.values\n",
    "                                date_enrol.drop(drop_index, inplace=True)                                            \n",
    "  \n",
    "                    for date2 in date_enrol.date.unique():                                                     \n",
    "                       # locate the row to be edited in adnicomb based on subject's ID and visit's date\n",
    "                        try:\n",
    "                            edit_boolean = np.logical_and(subj_date_merge[index1].iloc[:,0]==subject, subj_date_merge[index2].iloc[:,0]==date2)\n",
    "                            edit_row = adnicomb_index[edit_boolean].values\n",
    "                        except:\n",
    "                            edit_boolean = np.logical_and(subj_date_merge[index1]==subject, subj_date_merge[index2]==date2)\n",
    "                            edit_row = adnicomb_index[edit_boolean].values\n",
    "                            \n",
    "                        # selected row in csv. file of interest based on date_enrol\n",
    "                        enrol_row = date_enrol.date.loc[date_enrol.date==date2].index.values\n",
    "                        enrol_list = col_csv.loc[enrol_row][new_col].to_list()\n",
    "                        \n",
    "                        # If one single clinical follow-up corresponds to multiple records from the csv-of-interest,\n",
    "                        # all those records are warpped up into a list and then inserted to that follow-up's dataframe cell   \n",
    "                        if flag_dupl==1: \n",
    "                            #adnicomb.loc[edit_row, new_col] = enrol_list  \n",
    "                            #--> not working when trying to insert a 'list' into a dataframe cell\n",
    "                            \n",
    "                            adnicomb.loc[edit_row,new_col] = adnicomb.loc[edit_row, new_col].apply(lambda x: enrol_list)\n",
    "                        else:\n",
    "                            adnicomb.loc[edit_row,new_col] = adnicomb.loc[edit_row, new_col].apply(lambda x: enrol_list)\n",
    "                            \n",
    "                            #adnicomb.loc[edit_row, new_col] = enrol_list \n",
    "                            #--> it works, but we'd better keep all dataframe cells as type of 'list',\n",
    "                            # for convinience purposes in terms of future data calling\n",
    "                \n",
    "    end = time.time()        \n",
    "    print('    with using ', '{:.{prec}f}'.format(end -start, prec=2), ' seconds ...')        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unifying naming convention of brain regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dti_MD_CST_L\n",
      "MD\n",
      "dti_MD_CST_R\n",
      "MD\n",
      "dti_MD_ICP_L\n",
      "MD\n",
      "dti_MD_ICP_R\n",
      "MD\n",
      "dti_MD_ML_L\n",
      "MD\n",
      "dti_MD_ML_R\n",
      "MD\n",
      "dti_MD_SCP_L\n",
      "MD\n",
      "dti_MD_SCP_R\n",
      "MD\n",
      "dti_MD_CP_L\n",
      "MD\n",
      "dti_MD_CP_R\n",
      "MD\n",
      "dti_MD_ALIC_L\n",
      "MD\n",
      "dti_MD_ALIC_R\n",
      "MD\n",
      "dti_MD_PLIC_L\n",
      "MD\n",
      "dti_MD_PLIC_R\n",
      "MD\n",
      "dti_MD_PTR_L\n",
      "MD\n",
      "dti_MD_PTR_R\n",
      "MD\n",
      "dti_MD_ACR_L\n",
      "MD\n",
      "dti_MD_ACR_R\n",
      "MD\n",
      "dti_MD_SCR_L\n",
      "MD\n",
      "dti_MD_SCR_R\n",
      "MD\n",
      "dti_MD_PCR_L\n",
      "MD\n",
      "dti_MD_PCR_R\n",
      "MD\n",
      "dti_MD_CGC_L\n",
      "MD\n",
      "dti_MD_CGC_R\n",
      "MD\n",
      "dti_MD_CGH_L\n",
      "MD\n",
      "dti_MD_CGH_R\n",
      "MD\n",
      "dti_MD_FX_ST_L\n",
      "MD\n",
      "dti_MD_FX_ST_R\n",
      "MD\n",
      "dti_MD_SLF_L\n",
      "MD\n",
      "dti_MD_SLF_R\n",
      "MD\n",
      "dti_MD_SFO_L\n",
      "MD\n",
      "dti_MD_SFO_R\n",
      "MD\n",
      "dti_MD_IFO_L\n",
      "MD\n",
      "dti_MD_IFO_R\n",
      "MD\n",
      "dti_MD_SS_L\n",
      "MD\n",
      "dti_MD_SS_R\n",
      "MD\n",
      "dti_MD_EC_L\n",
      "MD\n",
      "dti_MD_EC_R\n",
      "MD\n",
      "dti_MD_UNC_L\n",
      "MD\n",
      "dti_MD_UNC_R\n",
      "MD\n",
      "dti_MD_FX_L\n",
      "MD\n",
      "dti_MD_FX_R\n",
      "MD\n",
      "dti_MD_GCC_L\n",
      "MD\n",
      "dti_MD_GCC_R\n",
      "MD\n",
      "dti_MD_BCC_L\n",
      "MD\n",
      "dti_MD_BCC_R\n",
      "MD\n",
      "dti_MD_SCC_L\n",
      "MD\n",
      "dti_MD_SCC_R\n",
      "MD\n",
      "dti_MD_RLIC_L\n",
      "MD\n",
      "dti_MD_RLIC_R\n",
      "MD\n",
      "dti_MD_TAP_L\n",
      "MD\n",
      "dti_MD_TAP_R\n",
      "MD\n",
      "dti_MD_SUMGCC\n",
      "MD\n",
      "dti_MD_SUMBCC\n",
      "MD\n",
      "dti_MD_SUMSCC\n",
      "MD\n",
      "dti_MD_SUMCC\n",
      "MD\n",
      "dti_MD_SUMFX\n",
      "MD\n"
     ]
    }
   ],
   "source": [
    "adnicomb = change_naming_convention(adnicomb, nm_pd, metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chang CerebellumCortex_SV etc. to CerebellumCortex_CV etc.\n",
    "for prefix in ['xv15one','xv30one','xv30two','xv3three']:\n",
    "    for region in ['CerebellumCortex', 'Thalamus','Caudate','Putamen',\n",
    "                   'Pallidum','Hippocampus','Amygdala','AccumbensArea', 'VentralDC']:\n",
    "                adnicomb.rename(\n",
    "                        columns={prefix+'_Right'+region+'_SV': prefix+'_Right'+region+'_CV'}, \n",
    "                        inplace=True)\n",
    "                adnicomb.rename(\n",
    "                        columns={prefix+'_Left'+region+'_SV': prefix+'_Left'+region+'_CV'}, \n",
    "                        inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving ADNICOMB as pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving generated dataframe to pickle\n",
    "adnicomb.to_pickle(working_dir+'/adnicomb_v1_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "adnicomb = pd.read_pickle(working_dir+'/adnicomb_v1_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taunpvc RID\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merge_RID</th>\n",
       "      <th>merge_EXAMDATE</th>\n",
       "      <th>merge_DX</th>\n",
       "      <th>taunpvc_RID</th>\n",
       "      <th>taunpvc_VISCODE</th>\n",
       "      <th>taunpvc_VISCODE2</th>\n",
       "      <th>taunpvc_EXAMDATE</th>\n",
       "      <th>taunpvc_InferiorCerebellum_SUVR</th>\n",
       "      <th>taunpvc_InferiorCerebellum_VOLUME</th>\n",
       "      <th>taunpvc_ErodedSubcorticalWM_SUVR</th>\n",
       "      <th>...</th>\n",
       "      <th>taunpvc_RightThalamus_SUVR</th>\n",
       "      <th>taunpvc_RightThalamus_VOLUME</th>\n",
       "      <th>taunpvc_RightVentralDC_SUVR</th>\n",
       "      <th>taunpvc_RightVentralDC_VOLUME</th>\n",
       "      <th>taunpvc_RightVessel_SUVR</th>\n",
       "      <th>taunpvc_RightVessel_VOLUME</th>\n",
       "      <th>taunpvc_WMHypoIntensities_SUVR</th>\n",
       "      <th>taunpvc_WMHypoIntensities_VOLUME</th>\n",
       "      <th>taunpvc_update_stamp</th>\n",
       "      <th>taunpvc_numdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>21</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>CN</td>\n",
       "      <td>[21]</td>\n",
       "      <td>[init]</td>\n",
       "      <td>[m144]</td>\n",
       "      <td>[2018-02-02]</td>\n",
       "      <td>[1.0299]</td>\n",
       "      <td>[52274]</td>\n",
       "      <td>[1.2078]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.4502]</td>\n",
       "      <td>[5968]</td>\n",
       "      <td>[1.37]</td>\n",
       "      <td>[3045]</td>\n",
       "      <td>[1.4225]</td>\n",
       "      <td>[55.0]</td>\n",
       "      <td>[1.0307]</td>\n",
       "      <td>[3229]</td>\n",
       "      <td>[2019-08-28 15:59:33.0]</td>\n",
       "      <td>[736727]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>31</td>\n",
       "      <td>2018-04-17</td>\n",
       "      <td>CN</td>\n",
       "      <td>[31]</td>\n",
       "      <td>[init]</td>\n",
       "      <td>[m144]</td>\n",
       "      <td>[2018-04-24]</td>\n",
       "      <td>[0.9597]</td>\n",
       "      <td>[47466]</td>\n",
       "      <td>[1.0607]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.1399]</td>\n",
       "      <td>[5442]</td>\n",
       "      <td>[1.2306]</td>\n",
       "      <td>[3083]</td>\n",
       "      <td>[1.3924]</td>\n",
       "      <td>[25.0]</td>\n",
       "      <td>[0.7766]</td>\n",
       "      <td>[31427]</td>\n",
       "      <td>[2019-08-28 15:59:33.0]</td>\n",
       "      <td>[736808]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>31</td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>CN</td>\n",
       "      <td>[31]</td>\n",
       "      <td>[y1]</td>\n",
       "      <td>[m156]</td>\n",
       "      <td>[2019-04-23]</td>\n",
       "      <td>[0.9620000000000001]</td>\n",
       "      <td>[47466]</td>\n",
       "      <td>[1.0461]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.0777]</td>\n",
       "      <td>[5442]</td>\n",
       "      <td>[1.1674]</td>\n",
       "      <td>[3083]</td>\n",
       "      <td>[1.4014]</td>\n",
       "      <td>[25.0]</td>\n",
       "      <td>[0.7909]</td>\n",
       "      <td>[31427]</td>\n",
       "      <td>[2019-08-28 15:59:33.0]</td>\n",
       "      <td>[737172]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>56</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>MCI</td>\n",
       "      <td>[56]</td>\n",
       "      <td>[init]</td>\n",
       "      <td>[m144]</td>\n",
       "      <td>[2018-02-20]</td>\n",
       "      <td>[0.9873]</td>\n",
       "      <td>[56231]</td>\n",
       "      <td>[1.2592]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.3359]</td>\n",
       "      <td>[6143]</td>\n",
       "      <td>[1.3548]</td>\n",
       "      <td>[3243]</td>\n",
       "      <td>[1.5292]</td>\n",
       "      <td>[98.0]</td>\n",
       "      <td>[1.035]</td>\n",
       "      <td>[2227]</td>\n",
       "      <td>[2019-08-28 15:59:33.0]</td>\n",
       "      <td>[736745]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>56</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>MCI</td>\n",
       "      <td>[56]</td>\n",
       "      <td>[y1]</td>\n",
       "      <td>[m156]</td>\n",
       "      <td>[2019-01-10]</td>\n",
       "      <td>[0.9621]</td>\n",
       "      <td>[56231]</td>\n",
       "      <td>[1.2903]</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.364]</td>\n",
       "      <td>[6143]</td>\n",
       "      <td>[1.3791]</td>\n",
       "      <td>[3243]</td>\n",
       "      <td>[1.4749]</td>\n",
       "      <td>[98.0]</td>\n",
       "      <td>[1.0708]</td>\n",
       "      <td>[2227]</td>\n",
       "      <td>[2019-08-28 15:59:33.0]</td>\n",
       "      <td>[737069]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     merge_RID merge_EXAMDATE merge_DX taunpvc_RID taunpvc_VISCODE  \\\n",
       "109         21     2017-11-27       CN        [21]          [init]   \n",
       "162         31     2018-04-17       CN        [31]          [init]   \n",
       "163         31     2019-04-23       CN        [31]            [y1]   \n",
       "323         56     2017-11-28      MCI        [56]          [init]   \n",
       "324         56     2019-01-10      MCI        [56]            [y1]   \n",
       "\n",
       "    taunpvc_VISCODE2 taunpvc_EXAMDATE taunpvc_InferiorCerebellum_SUVR  \\\n",
       "109           [m144]     [2018-02-02]                        [1.0299]   \n",
       "162           [m144]     [2018-04-24]                        [0.9597]   \n",
       "163           [m156]     [2019-04-23]            [0.9620000000000001]   \n",
       "323           [m144]     [2018-02-20]                        [0.9873]   \n",
       "324           [m156]     [2019-01-10]                        [0.9621]   \n",
       "\n",
       "    taunpvc_InferiorCerebellum_VOLUME taunpvc_ErodedSubcorticalWM_SUVR  ...  \\\n",
       "109                           [52274]                         [1.2078]  ...   \n",
       "162                           [47466]                         [1.0607]  ...   \n",
       "163                           [47466]                         [1.0461]  ...   \n",
       "323                           [56231]                         [1.2592]  ...   \n",
       "324                           [56231]                         [1.2903]  ...   \n",
       "\n",
       "    taunpvc_RightThalamus_SUVR taunpvc_RightThalamus_VOLUME  \\\n",
       "109                   [1.4502]                       [5968]   \n",
       "162                   [1.1399]                       [5442]   \n",
       "163                   [1.0777]                       [5442]   \n",
       "323                   [1.3359]                       [6143]   \n",
       "324                    [1.364]                       [6143]   \n",
       "\n",
       "    taunpvc_RightVentralDC_SUVR taunpvc_RightVentralDC_VOLUME  \\\n",
       "109                      [1.37]                        [3045]   \n",
       "162                    [1.2306]                        [3083]   \n",
       "163                    [1.1674]                        [3083]   \n",
       "323                    [1.3548]                        [3243]   \n",
       "324                    [1.3791]                        [3243]   \n",
       "\n",
       "    taunpvc_RightVessel_SUVR taunpvc_RightVessel_VOLUME  \\\n",
       "109                 [1.4225]                     [55.0]   \n",
       "162                 [1.3924]                     [25.0]   \n",
       "163                 [1.4014]                     [25.0]   \n",
       "323                 [1.5292]                     [98.0]   \n",
       "324                 [1.4749]                     [98.0]   \n",
       "\n",
       "    taunpvc_WMHypoIntensities_SUVR taunpvc_WMHypoIntensities_VOLUME  \\\n",
       "109                       [1.0307]                           [3229]   \n",
       "162                       [0.7766]                          [31427]   \n",
       "163                       [0.7909]                          [31427]   \n",
       "323                        [1.035]                           [2227]   \n",
       "324                       [1.0708]                           [2227]   \n",
       "\n",
       "        taunpvc_update_stamp taunpvc_numdate  \n",
       "109  [2019-08-28 15:59:33.0]        [736727]  \n",
       "162  [2019-08-28 15:59:33.0]        [736808]  \n",
       "163  [2019-08-28 15:59:33.0]        [737172]  \n",
       "323  [2019-08-28 15:59:33.0]        [736745]  \n",
       "324  [2019-08-28 15:59:33.0]        [737069]  \n",
       "\n",
       "[5 rows x 245 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check whether one certain csv. files has been imported accurately\n",
    "# (only non-NaN elements shown here)\n",
    "i=18\n",
    "temp_alias = comb_df.alias[i]\n",
    "temp_entry = comb_df.subject_entry[i]\n",
    "print(temp_alias, temp_entry)\n",
    "temp_pd = (adnicomb[['merge_RID','merge_EXAMDATE', 'merge_DX']]\n",
    "                   .join(adnicomb.filter(regex='^'+temp_alias, axis=1)))\n",
    "temp_pd = temp_pd.loc[temp_pd[temp_alias + '_' + temp_entry].notnull()]\n",
    "temp_pd.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
